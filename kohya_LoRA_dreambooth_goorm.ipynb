{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgjeYgd6pWp"
      },
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=linaqruf.lora-dreambooth)\n",
        "#Kohya LoRA Dreambooth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPgBR3KM6E-Z"
      },
      "source": [
        "본 파일은 해당 주피터 파일을 구름 버전으로 수정한 버전입니다.[Colab버전 Lora 학습](https://oo.pe/https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb#scrollTo=En9UUwGNMRMM)<br>\n",
        "<br>\n",
        "기본적으로 kohya-ss의 git 프로젝트를 기반으로 합니다.[kohya-ss/sd-script](https://github.com/kohya-ss/sd-scripts)<br>\n",
        "주피터 파일 버전을 제작한 제작자에게 감사드립니다.[Linaqruf](https://github.com/Linaqruf)<br>\n",
        "주피터 파일 원본 겸 최신파일은 다음 링크에서 확인할 수 있습니다.[here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb)<br><br>\n",
        "수행 전 아래의 명령어를 하단 터미널에 순서대로 넣어 필요한 것을 설치해야 동작합니다.<br>\n",
        "pip install --upgrade pip<br>\n",
        "pip install --upgrade torchvision==0.14.0<br>\n",
        "pip install prettytable<br>\n",
        "apt-get update<br>\n",
        "apt install -y libgl1-mesa-glx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYKhpD47vl_o"
      },
      "source": [
        "# 각종 주요 모델 링크(Ⅱ에서 필요시 사용)\n",
        "기본 설정: AbyssOrangeMix2_nsfw + VAE없음(해당 조합으로만 테스트 수행함)<br>\n",
        "유명 모델 허깅페이스 다운로드 URL(게시자에 의해 링크 주소가 변경될 수 있으므로 가급적 직접 확인 권장)<br>\n",
        "SD V1 모델<br>\n",
        "https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt<br>\n",
        "https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors<br>\n",
        "https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt<br>\n",
        "https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt<br>\n",
        "https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors<br>\n",
        "https://huggingface.co/gsdf/Counterfeit-V2.0/resolve/main/Counterfeit-V2.0fp16.safetensors<br>\n",
        "https://huggingface.co/closertodeath/dpepteahands3/resolve/main/dpepteahand3.ckpt<br>\n",
        "https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt<br>\n",
        "https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt<br>\n",
        "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt<br><br>\n",
        "SD V2 모델<br>\n",
        "https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt<br>\n",
        "https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.ckpt<br>\n",
        "https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e2.ckpt<br>\n",
        "https://huggingface.co/p1atdev/pd-archive/resolve/main/plat-v1-3-1.safetensors<br><br>\n",
        "VAE<br>\n",
        "https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt<br>\n",
        "https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt<br>\n",
        "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# I. 학습 필요 파일 설치\n",
        "root_dir: 주요파일이 들어갈 기본 루트폴더 경로<br>\n",
        "branch: 레포지토리 업데이트로 인한 에러가 발생할 경우 이전버전을 사용하기 위해 넣는 해시, 비어있다면 최신버전을 사용<br>\n",
        "install_xformers: xformers 설치 수행여부<br>\n",
        "**write_token: [허깅페이스 토큰](https://huggingface.co/settings/tokens)을 작성, 반드시 Role을 Write로 설정하여 생성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V02Didwvl_p"
      },
      "outputs": [],
      "source": [
        "root_dir = \"/workspace/repository\"\n",
        "branch = \"\"\n",
        "install_xformers = True\n",
        "write_token = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_u3q60di584x",
        "outputId": "0f482d21-84eb-40dd-8c92-036ed4efc869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Feb  3 04:32:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Stored 'root_dir' (str)\n",
            "Stored 'repo_dir' (str)\n",
            "Stored 'tools_dir' (str)\n",
            "Stored 'finetune_dir' (str)\n",
            "Stored 'training_dir' (str)\n",
            "[Errno 2] No such file or directory: '/workspace/repository'\n",
            "'/workspace/repository/kohya-trainer'에 복제합니다...\n",
            "remote: Enumerating objects: 1118, done.\u001b[K\n",
            "remote: Counting objects: 100% (540/540), done.\u001b[K\n",
            "remote: Compressing objects: 100% (291/291), done.\u001b[K\n",
            "remote: Total 1118 (delta 380), reused 350 (delta 249), pack-reused 578\u001b[K\n",
            "오브젝트를 받는 중: 100% (1118/1118), 2.81 MiB | 20.44 MiB/s, 완료.\n",
            "델타를 알아내는 중: 100% (684/684), 완료.\n",
            "/workspace/repository\n"
          ]
        }
      ],
      "source": [
        "# Kohya Trainer 설치 파트\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "%store root_dir\n",
        "repo_dir = str(root_dir)+\"/kohya-trainer\"\n",
        "%store repo_dir\n",
        "tools_dir = str(root_dir)+\"/kohya-trainer/tools\"\n",
        "%store tools_dir \n",
        "finetune_dir = str(root_dir)+\"/kohya-trainer/finetune\"\n",
        "%store finetune_dir\n",
        "training_dir = str(root_dir)+\"/dreambooth\"\n",
        "%store training_dir\n",
        "\n",
        "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
        "\n",
        "def clone_repo():\n",
        "  if os.path.isdir(repo_dir):\n",
        "    print(\"The repository folder already exists, will do a !git pull instead\\n\")\n",
        "    %cd {repo_dir}\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "  else:\n",
        "    %cd {root_dir}\n",
        "    !git clone {repo_url} {repo_dir}\n",
        "\n",
        "if not os.path.isdir(repo_dir):\n",
        "  clone_repo()\n",
        "\n",
        "%cd {root_dir}\n",
        "os.makedirs(repo_dir, exist_ok=True)\n",
        "os.makedirs(tools_dir, exist_ok=True)\n",
        "os.makedirs(finetune_dir, exist_ok=True)\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "\n",
        "if branch:\n",
        "  %cd {repo_dir}\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WNn0g1pnHfk5",
        "outputId": "02dc97c6-a523-4edb-e4be-2808757977ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'accelerate_config' (str)\n",
            "/workspace/repository/kohya-trainer\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9681 sha256=4769ad9a78617386829dacfcf8f6a56231e53a5f9e19b3a6463c015ccec6d740\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-03 04:36:04.029960: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-03 04:36:04.244440: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-02-03 04:36:05.373922: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-02-03 04:36:05.374138: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-02-03 04:36:05.374158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "# 추가 필요부분 설치 파트\n",
        "%store -r\n",
        "\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "%store accelerate_config\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "def install_dependencies():\n",
        "    !pip -qqqq install --upgrade gallery-dl gdown imjoy-elfinder\n",
        "    !python3 -m pip install wget\n",
        "    !pip -qqqq install --upgrade -r requirements.txt\n",
        "\n",
        "    if install_xformers:\n",
        "        !pip -qqqq install -U -I --no-deps https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "    if not os.path.exists(accelerate_config):\n",
        "        write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "install_dependencies()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rl2zERHbBQ9W",
        "outputId": "de574b6e-7e68-4662-9629-26c33fe9317c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid.\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Stored 'write_token' (str)\n"
          ]
        }
      ],
      "source": [
        "# 클라우드 서비스(허깅페이스) 로그인 파트\n",
        "# 왜 필요한지는 몰?루 아마 일부 파일을 허깅페이스에서 받을 때 쓰는 듯?\n",
        "from huggingface_hub import login\n",
        "%store -r\n",
        "\n",
        "login(write_token, add_to_git_credential=True)\n",
        "\n",
        "%store write_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gob9_OwTlwh"
      },
      "source": [
        "# II. Pretrained Model 다운로드\n",
        "**modelUrl: 다운로드할 모델 허깅페이스 다운로드 URL, 미기재시 다운로드 수행 안함**<br>\n",
        "modelName: 저장할 모델명, 확장자명 포함 작성<br><br>\n",
        "p_modelUrl: 개인 허깅 혹은 구글드라이브 다운로드 필요시 수행, 구드 사용시 **python3 -m pip install gdown** 터미널 작성하여 gdown 설치 필요<br><br>\n",
        "vaeUrl: VAE 허깅페이스 다운로드 URL, 미기재시 다운로드 수행 안함<br>\n",
        "vaeName:저장할 VAE명, 확장자명 포함 작성<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "magSko_uvl_t"
      },
      "outputs": [],
      "source": [
        "modelUrl = \"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors\"\n",
        "modelName = \"AbyssOrangeMix2_nsfw.safetensors\"\n",
        "\n",
        "p_modelUrl = \"\"\n",
        "\n",
        "vaeUrl = \"\"\n",
        "vaeName = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wmnsZwClN1XL",
        "outputId": "66a3ff73-665c-4fb7-ceef-5c9375d7a64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/repository\n",
            "--2023-02-03 04:43:09--  https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors\n",
            "huggingface.co (huggingface.co)을(를) 해석하는 중... 54.235.118.239, 3.231.67.228, 2600:1f18:147f:e850:e203:c458:10cd:fc3c, ...\n",
            "접속 huggingface.co (huggingface.co)|54.235.118.239|:443... 접속됨.\n",
            "HTTP 요청을 전송했습니다. 응답을 기다리는 중입니다... 302 Found\n",
            "위치: https://cdn-lfs.huggingface.co/repos/a3/2d/a32dc75b6b955f4b9a0498061b68d02fc24b9e596c8126c9817795c3f0356c18/0873291ac5419eaa7a18726e8841ce0f15f701ace29e0183c47efad2018900a4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27AbyssOrangeMix2_nsfw.safetensors%3B+filename%3D%22AbyssOrangeMix2_nsfw.safetensors%22%3B&Expires=1675658082&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzJkL2EzMmRjNzViNmI5NTVmNGI5YTA0OTgwNjFiNjhkMDJmYzI0YjllNTk2YzgxMjZjOTgxNzc5NWMzZjAzNTZjMTgvMDg3MzI5MWFjNTQxOWVhYTdhMTg3MjZlODg0MWNlMGYxNWY3MDFhY2UyOWUwMTgzYzQ3ZWZhZDIwMTg5MDBhND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzU2NTgwODJ9fX1dfQ__&Signature=tq6h76uKGs2HrpBhU5edj9EKCJj6NSAH31gssIEdhWvp6o2Ji3GDBlFBPdshFmMIOyWqJ8LU7HMkxUdFrtlgJsbIfo74UQhPV%7EwRxs5FaQzkcLk5bmMO33FC2w3pwGCifEoW6IzKI0WnmGhGbNXoXOvZlruytqqHBEtdWJSc14OaPAUqNLGzRwog5ORXEaSGLufNldwnlYpfu%7EAnpGFRweALmRAAEGANfusUfCi0eyinmDt3U-iblNj0tsCO%7ElKeE2SuLO0%7Eo3F5NI70r1m7YETHg4c0fKy4m-xT8iyiAWW801NmxSglH-NmuoRMPCdCmmUjfULLB7XjHLw4hFoGJw__&Key-Pair-Id=KVTP0A1DKRTAX [따라감]\n",
            "--2023-02-03 04:43:10--  https://cdn-lfs.huggingface.co/repos/a3/2d/a32dc75b6b955f4b9a0498061b68d02fc24b9e596c8126c9817795c3f0356c18/0873291ac5419eaa7a18726e8841ce0f15f701ace29e0183c47efad2018900a4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27AbyssOrangeMix2_nsfw.safetensors%3B+filename%3D%22AbyssOrangeMix2_nsfw.safetensors%22%3B&Expires=1675658082&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzJkL2EzMmRjNzViNmI5NTVmNGI5YTA0OTgwNjFiNjhkMDJmYzI0YjllNTk2YzgxMjZjOTgxNzc5NWMzZjAzNTZjMTgvMDg3MzI5MWFjNTQxOWVhYTdhMTg3MjZlODg0MWNlMGYxNWY3MDFhY2UyOWUwMTgzYzQ3ZWZhZDIwMTg5MDBhND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzU2NTgwODJ9fX1dfQ__&Signature=tq6h76uKGs2HrpBhU5edj9EKCJj6NSAH31gssIEdhWvp6o2Ji3GDBlFBPdshFmMIOyWqJ8LU7HMkxUdFrtlgJsbIfo74UQhPV%7EwRxs5FaQzkcLk5bmMO33FC2w3pwGCifEoW6IzKI0WnmGhGbNXoXOvZlruytqqHBEtdWJSc14OaPAUqNLGzRwog5ORXEaSGLufNldwnlYpfu%7EAnpGFRweALmRAAEGANfusUfCi0eyinmDt3U-iblNj0tsCO%7ElKeE2SuLO0%7Eo3F5NI70r1m7YETHg4c0fKy4m-xT8iyiAWW801NmxSglH-NmuoRMPCdCmmUjfULLB7XjHLw4hFoGJw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)을(를) 해석하는 중... 54.239.168.32, 54.239.168.66, 54.239.168.128, ...\n",
            "접속 cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|54.239.168.32|:443... 접속됨.\n",
            "HTTP 요청을 전송했습니다. 응답을 기다리는 중입니다... 200 OK\n",
            "길이: 5570803627 (5.2G) [binary/octet-stream]\n",
            "다음 위치에 저장: `/workspace/repository/pre_trained_model/AbyssOrangeMix2_nsfw.safetensors'\n",
            "\n",
            "/workspace/reposito 100%[===================>]   5.19G  15.5MB/s    / 2m 40s   \n",
            "\n",
            "2023-02-03 04:45:50 (33.2 MB/s) - `/workspace/repository/pre_trained_model/AbyssOrangeMix2_nsfw.safetensors' 저장됨 [5570803627/5570803627]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 허깅페이스를 통한 모델 다운로드\n",
        "import wget\n",
        "\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installModels = []\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
        "  target = root_dir+'/pre_trained_model/'\n",
        "  os.makedirs(target, exist_ok=True)\n",
        "  target = root_dir+'/pre_trained_model/'+checkpoint_name\n",
        "  hf_token = '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' \n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !wget {url} -O {target} --header {user_header}\n",
        "\n",
        "def install_checkpoint():\n",
        "  for model in installModels:\n",
        "    install(model[0], model[1])\n",
        "if modelName != \"\":\n",
        "  installModels.append((modelName, modelUrl))\n",
        "  install_checkpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3LWn6GzNQ4j5",
        "outputId": "45229787-53ad-469e-a1ec-9b6e42d16e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/repository\n"
          ]
        }
      ],
      "source": [
        "# 그 외 모델 다운로드\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "dst = str(root_dir)+\"/pre_trained_model\"\n",
        "\n",
        "if not os.path.exists(dst):\n",
        "    os.makedirs(dst)\n",
        "\n",
        "def install(url):\n",
        "  base_name = os.path.basename(url)\n",
        "\n",
        "  if url.startswith(\"https://drive.google.com\"):\n",
        "    %cd {dst}\n",
        "    !gdown --fuzzy {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    # 개인용 모델 다운에 따로 토큰 쓰라는데 그냥 앞에 적힌거로 묶음, 잘 아는사람 있으면 물어보던지 알아서 깔겠지 뭐\n",
        "    hf_token = write_token\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !wget {url} -O {base_name} --header {user_header}\n",
        "  else:\n",
        "    !wget -P {dst} {url}\n",
        "\n",
        "if p_modelUrl != \"\":\n",
        "    install(p_modelUrl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SoucgZQ6jgPQ",
        "outputId": "9374f9d7-0153-4e2a-f763-3a70b97370ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/repository\n"
          ]
        }
      ],
      "source": [
        "# VAE 다운로드\n",
        "%store -r \n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installVae = []\n",
        "\n",
        "installVae.append((vaeName, vaeUrl))\n",
        "\n",
        "def install(vae_name, url):\n",
        "  hf_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  target = root_dir+'/vae/'+vae_name\n",
        "  !wget {url} -O {target} --header {user_header}\n",
        "    \n",
        "def install_vae():\n",
        "  if vaeName != \"\":\n",
        "    for vae in installVae:\n",
        "      install(vae[0], vae[1])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "install_vae()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En9UUwGNMRMM"
      },
      "source": [
        "# Ⅲ. Data 습득\n",
        "하단 부분은 반복횟수 제외 폴더명 생성에만 사용함, 학습 관련 내용은 파트Ⅴ에서 지정\n",
        "train_folder_directory: 학습 폴더 기본 경로<br>\n",
        "train_repeats: 학습 폴더명 사용, 반복횟수<br>\n",
        "concept_name: 학습 폴더명 사용<br>\n",
        "reg_repeats: 정규화 폴더명 사용, 반복횟수<br>\n",
        "class_name: 정규화 폴더명 사용<br>\n",
        "학습 폴더명 형식 = {train_repeats}_{concept_name} {class_name}<br>\n",
        "정규화 폴더명 형식 = {reg_repeats}_{class_name}<br><br>\n",
        "\n",
        "이미지 및 태그 자동 크롤링 기능 설정, 사용시 작업폴더에 자동으로 이미지와 태그를 집어넣어줌<br>\n",
        "단, 해당 태그에 맞춰 자동으로 죄다 긁어오므로 원하는 컨셉과 맞지 않거나 퀼리티가 낮아 학습에 악영향을 줄 수 있으므로 **가급적 학습데이터는 직접 준비하는걸 권장**<br>\n",
        "booru_down: 부루 사이트에서 이미지 크롤링 사용 여부<br>\n",
        "booru: \"Danbooru\", \"Gelbooru\" 둘 중 하나 골라 넣기<br>\n",
        "tag1: 찾을 태그명<br>\n",
        "tag2: 찾을 2차 태그명(없어도 됨)<br>\n",
        "download_tags: 태그 같이 크롤링 여부(.txt 형식에 들어감)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H96Q7mBevl_u"
      },
      "outputs": [],
      "source": [
        "train_folder_directory = \"/workspace/repository/dreambooth/train_data\"\n",
        "reg_repeats = 1\n",
        "train_repeats = 10\n",
        "concept_name = \"BA_shiroko\"\n",
        "class_name = \"1girl\"\n",
        "\n",
        "booru_down = False\n",
        "booru = \"Gelbooru\"\n",
        "tag1 = \"darkest_dungeon\"\n",
        "tag2 = \"\"\n",
        "download_tags = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kh7CeDqK4l3Y",
        "outputId": "f43f24b1-83c6-4d92-e716-d3834b904ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'train_folder_directory' (str)\n",
            "Stored 'reg_folder_directory' (str)\n"
          ]
        }
      ],
      "source": [
        "# 학습 폴더 생성 및 지정\n",
        "%store -r\n",
        "\n",
        "%store train_folder_directory\n",
        "reg_folder_directory = os.path.join(os.path.dirname(train_folder_directory), \"reg_data\")\n",
        "%store reg_folder_directory\n",
        "\n",
        "def get_folder_name(repeats, class_name, concept_name=None):\n",
        "  if class_name:\n",
        "    return f\"{repeats}_{concept_name} {class_name}\" if concept_name else f\"{repeats}_{class_name}\"\n",
        "  return f\"{repeats}_{concept_name}\"\n",
        "\n",
        "train_folder = get_folder_name(train_repeats, class_name, concept_name=concept_name)\n",
        "reg_folder = get_folder_name(reg_repeats, class_name)\n",
        "\n",
        "train_data_dir = os.path.join(train_folder_directory, train_folder)\n",
        "reg_data_dir = os.path.join(reg_folder_directory, reg_folder)\n",
        "\n",
        "os.makedirs(train_folder_directory, exist_ok=True)\n",
        "os.makedirs(reg_folder_directory, exist_ok=True)\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "os.makedirs(reg_data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A0t1dfnU5Xkq"
      },
      "outputs": [],
      "source": [
        "# 부루 Scraper\n",
        "# 해당 태그에 맞는 이미지 자동으로 크롤링함\n",
        "if booru_down == True:\n",
        "    import html\n",
        "    %store -r\n",
        "\n",
        "    %cd {root_dir}\n",
        "\n",
        "    if tag2 != \"\":\n",
        "        tags = tag1 + \"+\" + tag2\n",
        "    else:\n",
        "        tags = tag1\n",
        "\n",
        "    write_tags = \"--write-tags\" if download_tags == True else \"\"\n",
        "\n",
        "    if booru.lower() == \"danbooru\":\n",
        "        !gallery-dl \"https://danbooru.donmai.us/posts?tags={tags}\" {write_tags} -D \"{train_data_dir}\"\n",
        "    elif booru.lower() == \"gelbooru\":\n",
        "        !gallery-dl \"https://gelbooru.com/index.php?page=post&s=list&tags={tags}\" {write_tags} -D \"{train_data_dir}\"\n",
        "    else:\n",
        "        print(f\"Unknown booru site: {booru}\")\n",
        "\n",
        "    if download_tags == True: \n",
        "        files = [f for f in os.listdir(train_data_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "        for file in files:\n",
        "            file_path = os.path.join(train_data_dir, file)\n",
        "\n",
        "            with open(file_path, \"r\") as f:\n",
        "                contents = f.read()\n",
        "\n",
        "            contents = html.unescape(contents)\n",
        "            contents = contents.replace(\"_\", \" \")\n",
        "            contents = \", \".join(contents.split(\"\\n\"))\n",
        "\n",
        "            with open(file_path, \"w\") as f:\n",
        "                f.write(contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUiL2sLg7swG"
      },
      "source": [
        "# IV. Data 전처리\n",
        "start_labeling: 자동으로 caption 혹은 txt파일 생성, \"BLIP_Captioning\", \"WD_1_4_Tagger\" 둘 중 하나만 기재, 미기재시 미수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvdVLjbevl_v"
      },
      "outputs": [],
      "source": [
        "start_labeling = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ug648uiOvUZn",
        "outputId": "37e7b622-488d-40ec-be72-fee697406d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/repository\n"
          ]
        }
      ],
      "source": [
        "# 데이터 정리\n",
        "# 학습 데이터폴더에서 필요없는 데이터 정리, 원본은 메타데이터 삭제여부 뒀으나 그냥 남기도록 함\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "test = os.listdir(train_data_dir)\n",
        "\n",
        "supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".caption\", \".npz\", \".txt\", \".json\"]\n",
        "\n",
        "for item in test:\n",
        "    file_ext = os.path.splitext(item)[1]\n",
        "    if file_ext not in supported_types:\n",
        "        print(f\"Deleting file {item} from {train_data_dir}\")\n",
        "        os.remove(os.path.join(train_data_dir, item))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nvPyH-G_Qdha",
        "outputId": "b9db88e7-5c16-4448-b5ba-cf5d52c9efa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/repository/kohya-trainer/finetune\n"
          ]
        }
      ],
      "source": [
        "# 주석 처리\n",
        "# 자동으로 .caption 혹은 .txt 파일 생성\n",
        "%cd {finetune_dir}\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "if start_labeling == \"BLIP_Captioning\":\n",
        "  !python make_captions.py \\\n",
        "    \"{train_data_dir}\" \\\n",
        "    --batch_size {batch_size} \\\n",
        "    --caption_extension .caption\n",
        "elif start_labeling == \"WD_1_4_Tagger\":\n",
        "  !python tag_images_by_wd14_tagger.py \\\n",
        "    \"{train_data_dir}\" \\\n",
        "    --batch_size {batch_size} \\\n",
        "    --caption_extension .txt\n",
        "else:\n",
        "  pass\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HG0sGqZvl_w"
      },
      "source": [
        "# Ⅴ. 모델 학습\n",
        "v2: SD V2 여부, True 혹은 False로 작성<br>\n",
        "v_parameterization: 위와 동일<br><br>\n",
        "**project_name: Lora 이름**<br>\n",
        "**pretrained_model_name_or_path: Lora 학습에 사용할 모델 경로**<br>\n",
        "vae: Lora 학습에 사용할 VAE 경로<br>\n",
        "train_folder_directory: 학습 데이터 폴더가 들어있는 경로<br>\n",
        "reg_folder_directory: 정규화 데이터 폴더가 들어있는 경로<br>\n",
        "output_dir: 출력물 경로"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihtLrAKdvl_w"
      },
      "outputs": [],
      "source": [
        "v2 = False\n",
        "v_parameterization = False\n",
        "\n",
        "project_name = \"BA_shiroko\"\n",
        "pretrained_model_name_or_path = \"/workspace/repository/pre_trained_model/AbyssOrangeMix2_nsfw.safetensors\"\n",
        "vae = \"\"\n",
        "train_folder_directory = \"/workspace/repository/dreambooth/train_data\"\n",
        "reg_folder_directory = \"/workspace/repository/dreambooth/reg_data\"\n",
        "output_dir = \"/workspace/repository/dreambooth/output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQowGPFmvl_x"
      },
      "source": [
        "network_dim: LoRA rank값, 2배수로 사용, 클수록 출력 파일 크기 증가<br>\n",
        "network_alpha: LoRA α값, network_dim과 동일값 사용 혹은 작은 값 사용시 학습률(learning rate)을 크게 줘야함<br>\n",
        "network_weights: 'network_weights' can be specified to resume training 이라는데 잘 몰?루<br>\n",
        "network_train_on: Unet과 텍스트 인코더 학습 중 수행할 것, \"both\", \"unet_only\", \"text_encoder_only\" 중 기입<br>\n",
        "learning_rate: 전체 학습률<br>\n",
        "unet_lr: Unet 학습률<br>\n",
        "text_encoder_lr: 텍스트 인코더 학습률, Unet의 절반 권장<br>\n",
        "lr_scheduler: 학습률 과정중 변화방식, \"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\" 중 기입<br><br>\n",
        "lr_scheduler_num_cycles: lr_scheduler = cosine_with_restarts' 일 경우 사이클 횟수<br>\n",
        "lr_scheduler_power: lr_scheduler = polynomial' 일 경우 지정하는 값<br><br>\n",
        "no_metadata: 출력 모델에 메타데이터(주석) 저장 여부<br>\n",
        "training_comment: 메타데이터에 작성할 내용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1z72TCMvl_x"
      },
      "outputs": [],
      "source": [
        "network_dim = 128\n",
        "network_alpha = 128\n",
        "network_weights = \"\"\n",
        "network_train_on = \"both\"\n",
        "\n",
        "learning_rate = 1e-4\n",
        "unet_lr = 0\n",
        "text_encoder_lr = 5e-5\n",
        "lr_scheduler = \"constant\"\n",
        "\n",
        "lr_scheduler_num_cycles = 1\n",
        "lr_scheduler_power = 1\n",
        "\n",
        "no_metadata = False\n",
        "training_comment = \"this comment will be stored in the metadata\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg7ISC8jvl_x"
      },
      "source": [
        "train_batch_size: 배치 사이즈, 높을수록 속도 증가하나 퀼이 떨어지거나 out mem 에러 발생할 수 있음<br>\n",
        "num_epochs: 에폭 반복횟수<br>\n",
        "caption_extension: 캡션 확장자명<br><br>\n",
        "mixed_precision: 글카 따라 bf16 지원시 사용 가능, 구름환경에서는fp16<br>\n",
        "save_precision: 위와 동일<br><br>\n",
        "save_n_epochs_type: 특정 epoch에서 저장 방식, \"save_every_n_epochs\"(n번 epoch마다), \"save_n_epoch_ratio\"(전체중 n번 저장) 중 기입<br>\n",
        "save_n_epochs_type_value: 위 저장 방식의 n값<br><br>\n",
        "save_model_as: 저장 모델 확장자, \"ckpt\", \"pt\", \"safetensors\" 중 기입<br>\n",
        "resolution: 학습 해상도, 구름 환경 512 권장<br>\n",
        "max_token_length: 캡션 토큰 최대수, 75/150/225 중 기입<br>\n",
        "clip_skip: SD기반은 1, NAI기반은 2 기입(대부분 2)<br><br>\n",
        "use_8bit_adam: 최적화 설정, 구름환경에서는 사용 권장<br>\n",
        "gradient_checkpointing: 잘 몰?루 일반적으로 언급 안되니 안해도 될듯?<br>\n",
        "gradient_accumulation_steps: 위에꺼 사용시 쓰는 수치같음<br><br>\n",
        "seed: 시드값, 0이면 랜덤시드, 값에따라 같은 데이터와 세팅으로 결과 달라질 수 있음<br>\n",
        "additional_argument: 추가 인자<br>\n",
        "print_hyperparameter: 사용 파라미터 출력하기 여부<br>\n",
        "<br>\n",
        "**전체 학습 수행횟수** = 반복횟수(폴더명) x eposhs x 데이터갯수 / batch_size<br>\n",
        "ex) 학습데이터 폴더가 10_이름 형식(반복횟수 10), epoch 5, 학습데이터 50, batch size = 4인 경우<br>\n",
        "10x5x50/4=625 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s7wyLP3vl_x"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 4\n",
        "num_epochs = 10\n",
        "caption_extension = '.txt'\n",
        "\n",
        "mixed_precision = \"fp16\"\n",
        "save_precision = \"fp16\"\n",
        "\n",
        "save_n_epochs_type = \"save_n_epoch_ratio\"\n",
        "save_n_epochs_type_value = 5\n",
        "\n",
        "save_model_as = \"safetensors\"\n",
        "resolution = 512\n",
        "max_token_length = 225\n",
        "clip_skip = 2\n",
        "\n",
        "use_8bit_adam = True\n",
        "gradient_checkpointing = False\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "seed = 0\n",
        "additional_argument = \"--shuffle_caption --enable_bucket --xformers --cache_latents\"\n",
        "print_hyperparameter = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H_Q23fUEJhnC",
        "outputId": "7ba97696-7899-44c0-9dd9-390215a92713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'train_folder_directory' (str)\n",
            "Stored 'reg_folder_directory' (str)\n"
          ]
        }
      ],
      "source": [
        "# 주요 폴더 지정\n",
        "%store -r\n",
        "\n",
        "train_folder_directory = train_folder_directory\n",
        "%store train_folder_directory\n",
        "reg_folder_directory = reg_folder_directory\n",
        "%store reg_folder_directory\n",
        "resume_path =\"\"\n",
        "inference_url = \"https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "if v2 and not v_parameterization:\n",
        "  inference_url += \"v2-inference.yaml\"\n",
        "if v2 and v_parameterization:\n",
        "  inference_url += \"v2-inference-v.yaml\"\n",
        "\n",
        "try:\n",
        "  if v2:\n",
        "    !wget {inference_url} -O {output_dir}/{project_name}.yaml\n",
        "    print(\"File successfully downloaded\")\n",
        "except:\n",
        "  print(\"There was an error downloading the file. Please check the URL and try again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5P-QVvHMUrFB",
        "outputId": "36e14b33-3960-4cc6-f148-17c7f11894fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading network module: networks.lora\n",
            "networks.lora dim set to: 128\n",
            "networks.lora alpha set to: 128\n",
            "No LoRA weight loaded.\n",
            "Enabling LoRA for U-Net\n",
            "Enabling LoRA for Text Encoder\n",
            "UNet learning rate:  0.0001\n",
            "Text encoder learning rate:  5e-05\n",
            "Learning rate Scheduler: constant\n",
            "Training comment: this comment will be stored in the metadata\n"
          ]
        }
      ],
      "source": [
        "# Lora(Low Rank Adaptation) 학습에 사용할 파라미터 지정\n",
        "%store -r\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "\n",
        "print(\"Loading network module:\", network_module)\n",
        "print(f\"{network_module} dim set to:\", network_dim)\n",
        "print(f\"{network_module} alpha set to:\", network_alpha)\n",
        "\n",
        "if network_weights == \"\":\n",
        "  print(\"No LoRA weight loaded.\")\n",
        "else:\n",
        "  if os.path.exists(network_weights):\n",
        "    print(\"Loading LoRA weight:\", network_weights)\n",
        "  else:\n",
        "    print(f\"{network_weights} does not exist.\")\n",
        "    network_weights =\"\"\n",
        "\n",
        "if network_train_on == \"unet_only\":\n",
        "  print(\"Enabling LoRA for U-Net.\")\n",
        "  print(\"Disabling LoRA for Text Encoder.\")\n",
        "\n",
        "if network_train_on == \"unet_only\":\n",
        "  print(\"Enable LoRA for U-Net\")\n",
        "  print(\"Disable LoRA for Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "if network_train_on == \"text_encoder_only\":\n",
        "  print(\"Disabling LoRA for U-Net\")\n",
        "  print(\"Enabling LoRA for Text Encoder\")\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "else:\n",
        "  print(\"Enabling LoRA for U-Net\")\n",
        "  print(\"Enabling LoRA for Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "\n",
        "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
        "\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "  print(\"- Number of cycles: \", lr_scheduler_num_cycles)\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "  print(\"- Power: \", lr_scheduler_power)\n",
        "\n",
        "# Printing the training comment if metadata is not disabled and a comment is present\n",
        "if not no_metadata:\n",
        "  if training_comment: \n",
        "    print(\"Training comment:\", training_comment)\n",
        "else:\n",
        "  print(\"Metadata won't be saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GIvfcS1pOrGY",
        "outputId": "8b078a51-0ec0-4cde-c64b-52ff3970f662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/repository/kohya-trainer\n",
            "+-------------------------------+--------------------------------------------------------------------------+\n",
            "| Hyperparameter                | Value                                                                    |\n",
            "+-------------------------------+--------------------------------------------------------------------------+\n",
            "| v2                            | False                                                                    |\n",
            "| v_parameterization            | False                                                                    |\n",
            "| network_dim                   | 128                                                                      |\n",
            "| network_alpha                 | 128                                                                      |\n",
            "| network_module                | networks.lora                                                            |\n",
            "| network_weights               | False                                                                    |\n",
            "| network_train_on              | both                                                                     |\n",
            "| unet_lr                       | 0.0001                                                                   |\n",
            "| text_encoder_lr               | 5e-05                                                                    |\n",
            "| no_metadata                   | False                                                                    |\n",
            "| training_comment              | this comment will be stored in the metadata                              |\n",
            "| lr_scheduler                  | constant                                                                 |\n",
            "| lr_scheduler_num_cycles       | 1                                                                        |\n",
            "| lr_scheduler_power            | 1                                                                        |\n",
            "| pretrained_model_name_or_path | /workspace/repository/pre_trained_model/AbyssOrangeMix2_nsfw.safetensors |\n",
            "| vae                           | False                                                                    |\n",
            "| caption_extension             | .txt                                                                     |\n",
            "| train_folder_directory        | /workspace/repository/dreambooth/train_data                              |\n",
            "| reg_folder_directory          | /workspace/repository/dreambooth/reg_data                                |\n",
            "| output_dir                    | /workspace/repository/dreambooth/output                                  |\n",
            "| prior_loss_weight             | 1.0                                                                      |\n",
            "| resume_path                   | False                                                                    |\n",
            "| project_name                  | BA_shiroko                                                               |\n",
            "| mixed_precision               | fp16                                                                     |\n",
            "| save_precision                | fp16                                                                     |\n",
            "| save_n_epochs_type            | save_n_epoch_ratio                                                       |\n",
            "| save_n_epochs_type_value      | 5                                                                        |\n",
            "| save_model_as                 | safetensors                                                              |\n",
            "| resolution                    | 512                                                                      |\n",
            "| train_batch_size              | 4                                                                        |\n",
            "| max_token_length              | 225                                                                      |\n",
            "| use_8bit_adam                 | True                                                                     |\n",
            "| num_epochs                    | 10                                                                       |\n",
            "| seed                          | 0                                                                        |\n",
            "| gradient_checkpointing        | False                                                                    |\n",
            "| gradient_accumulation_steps   | 1                                                                        |\n",
            "| clip_skip                     | 2                                                                        |\n",
            "| logging_dir                   | /workspace/repository/dreambooth/logs                                    |\n",
            "| log_prefix                    | BA_shiroko                                                               |\n",
            "| additional_argument           | --shuffle_caption --enable_bucket --xformers --cache_latents             |\n",
            "+-------------------------------+--------------------------------------------------------------------------+\n",
            "2023-02-03 04:53:18.929697: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-03 04:53:19.161859: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-02-03 04:53:20.206721: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-02-03 04:53:20.206901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-02-03 04:53:20.206931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-03 04:53:23.485553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-03 04:53:23.697856: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-02-03 04:53:24.518832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-02-03 04:53:24.518987: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-02-03 04:53:24.519022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "prepare tokenizer\n",
            "Downloading (…)olve/main/vocab.json: 100%|████| 961k/961k [00:01<00:00, 883kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100%|████| 525k/525k [00:00<00:00, 673kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████| 389/389 [00:00<00:00, 148kB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████| 905/905 [00:00<00:00, 337kB/s]\n",
            "update token length: 225\n",
            "Use DreamBooth method.\n",
            "prepare train images.\n",
            "found directory 10_BA_shiroko 1girl contains 40 image files\n",
            "400 train images with repeating.\n",
            "prepare reg images.\n",
            "found directory 1_1girl contains 0 image files\n",
            "0 reg images.\n",
            "no regularization images / 正則化画像が見つかりませんでした\n",
            "loading image sizes.\n",
            "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 339.78it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (256, 832), count: 0\n",
            "bucket 1: resolution (256, 896), count: 0\n",
            "bucket 2: resolution (256, 960), count: 0\n",
            "bucket 3: resolution (256, 1024), count: 0\n",
            "bucket 4: resolution (320, 704), count: 70\n",
            "bucket 5: resolution (320, 768), count: 0\n",
            "bucket 6: resolution (384, 640), count: 180\n",
            "bucket 7: resolution (448, 576), count: 130\n",
            "bucket 8: resolution (512, 512), count: 10\n",
            "bucket 9: resolution (576, 448), count: 10\n",
            "bucket 10: resolution (640, 384), count: 0\n",
            "bucket 11: resolution (704, 320), count: 0\n",
            "bucket 12: resolution (768, 320), count: 0\n",
            "bucket 13: resolution (832, 256), count: 0\n",
            "bucket 14: resolution (896, 256), count: 0\n",
            "bucket 15: resolution (960, 256), count: 0\n",
            "bucket 16: resolution (1024, 256), count: 0\n",
            "mean ar error (without repeats): 0.04568883450503054\n",
            "prepare accelerator\n",
            "Using accelerator 0.15.0 or above.\n",
            "load StableDiffusion checkpoint\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Downloading (…)lve/main/config.json: 100%|██| 4.52k/4.52k [00:00<00:00, 449kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100%|██| 1.71G/1.71G [00:13<00:00, 129MB/s]\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'logit_scale', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'text_projection.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'visual_projection.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading text encoder: <All keys matched successfully>\n",
            "Replace CrossAttention.forward to use xformers\n",
            "caching latents.\n",
            "100%|███████████████████████████████████████████| 40/40 [00:26<00:00,  1.51it/s]\n",
            "import network module: networks.lora\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "prepare optimizer, data loader etc.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/python3.8/dist-packages/cv2/../../lib64')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:105: UserWarning: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64: did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('df ')}\n",
            "  warn(\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 114\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda114.so...\n",
            "use 8-bit Adam optimizer\n",
            "override steps. steps for 10 epochs is / 指定エポックまでのステップ数: 1020\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 400\n",
            "  num reg images / 正則化画像の数: 0\n",
            "  num batches per epoch / 1epochのバッチ数: 102\n",
            "  num epochs / epoch数: 10\n",
            "  batch size per device / バッチサイズ: 4\n",
            "  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: 4\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 1020\n",
            "steps:   0%|                                           | 0/1020 [00:00<?, ?it/s]epoch 1/10\n",
            "steps:  10%|██                   | 102/1020 [01:55<17:15,  1.13s/it, loss=0.146]epoch 2/10\n",
            "steps:  20%|████▏                | 204/1020 [03:49<15:18,  1.13s/it, loss=0.141]saving checkpoint: /workspace/repository/dreambooth/output/BA_shiroko-000002.safetensors\n",
            "epoch 3/10\n",
            "steps:  30%|██████▎              | 306/1020 [05:47<13:30,  1.13s/it, loss=0.138]epoch 4/10\n",
            "steps:  40%|████████▍            | 408/1020 [07:41<11:32,  1.13s/it, loss=0.138]saving checkpoint: /workspace/repository/dreambooth/output/BA_shiroko-000004.safetensors\n",
            "epoch 5/10\n",
            "steps:  50%|██████████▌          | 510/1020 [09:36<09:36,  1.13s/it, loss=0.125]epoch 6/10\n",
            "steps:  60%|████████████▌        | 612/1020 [11:31<07:40,  1.13s/it, loss=0.129]saving checkpoint: /workspace/repository/dreambooth/output/BA_shiroko-000006.safetensors\n",
            "epoch 7/10\n",
            "steps:  70%|██████████████▋      | 714/1020 [13:27<05:45,  1.13s/it, loss=0.123]epoch 8/10\n",
            "steps:  80%|████████████████▊    | 816/1020 [15:21<03:50,  1.13s/it, loss=0.129]saving checkpoint: /workspace/repository/dreambooth/output/BA_shiroko-000008.safetensors\n",
            "epoch 9/10\n",
            "steps:  90%|██████████████████▉  | 918/1020 [17:16<01:55,  1.13s/it, loss=0.116]epoch 10/10\n",
            "steps: 100%|████████████████████| 1020/1020 [19:10<00:00,  1.13s/it, loss=0.119]save trained model to /workspace/repository/dreambooth/output/BA_shiroko.safetensors\n",
            "model saved.\n",
            "steps: 100%|████████████████████| 1020/1020 [19:11<00:00,  1.13s/it, loss=0.119]\n"
          ]
        }
      ],
      "source": [
        "# Lora 수행 파라미터 지정 및 수행\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "import textwrap\n",
        "import yaml\n",
        "\n",
        "%store -r\n",
        "\n",
        "logging_dir = \"/workspace/repository/dreambooth/logs\"\n",
        "log_prefix = project_name\n",
        "prior_loss_weight =1.0\n",
        "%cd {repo_dir}\n",
        "\n",
        "train_command=f\"\"\"\n",
        "accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 train_network.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --network_dim={network_dim} \\\n",
        "  --network_alpha={network_alpha} \\\n",
        "  --network_module={network_module} \\\n",
        "  {\"--network_weights=\" + network_weights if network_weights else \"\"} \\\n",
        "  {\"--network_train_unet_only\" if network_train_on == \"unet_only\" else \"\"} \\\n",
        "  {\"--network_train_text_encoder_only\" if network_train_on == \"text_encoder_only\" else \"\"} \\\n",
        "  --learning_rate={learning_rate} \\\n",
        "  {\"--unet_lr=\" + format(unet_lr) if unet_lr !=0 else \"\"} \\\n",
        "  {\"--text_encoder_lr=\" + format(text_encoder_lr) if text_encoder_lr else \"\"} \\\n",
        "  {\"--no_metadata\" if no_metadata else \"\"} \\\n",
        "  {\"--training_comment=\" + \"training_comment\" if training_comment and not no_metadata else \"\"} \\\n",
        "  --lr_scheduler={lr_scheduler} \\\n",
        "  {\"--lr_scheduler_num_cycles=\" + format(lr_scheduler_num_cycles) if lr_scheduler == \"cosine_with_restarts\" else \"\"} \\\n",
        "  {\"--lr_scheduler_power=\" + format(lr_scheduler_power) if lr_scheduler == \"polynomial\" else \"\"} \\\n",
        "  --pretrained_model_name_or_path={pretrained_model_name_or_path} \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  --caption_extension={caption_extension} \\\n",
        "  --train_data_dir={train_folder_directory} \\\n",
        "  --reg_data_dir={reg_folder_directory} \\\n",
        "  --output_dir={output_dir} \\\n",
        "  --prior_loss_weight={prior_loss_weight} \\\n",
        "  {\"--resume=\" + resume_path if resume_path else \"\"} \\\n",
        "  {\"--output_name=\" + project_name if project_name else \"\"} \\\n",
        "  --mixed_precision={mixed_precision} \\\n",
        "  --save_precision={save_precision} \\\n",
        "  {\"--save_every_n_epochs=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_every_n_epochs\" else \"\"} \\\n",
        "  {\"--save_n_epoch_ratio=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_n_epoch_ratio\" else \"\"} \\\n",
        "  --save_model_as={save_model_as} \\\n",
        "  --resolution={resolution} \\\n",
        "  --train_batch_size={train_batch_size} \\\n",
        "  --max_token_length={max_token_length} \\\n",
        "  {\"--use_8bit_adam\" if use_8bit_adam else \"\"} \\\n",
        "  --max_train_epochs={num_epochs} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  {\"--gradient_checkpointing\" if gradient_checkpointing else \"\"} \\\n",
        "  {\"--gradient_accumulation_steps=\" + format(gradient_accumulation_steps) } \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --logging_dir={logging_dir} \\\n",
        "  --log_prefix={log_prefix} \\\n",
        "  {additional_argument}\n",
        "  \"\"\"\n",
        "\n",
        "debug_params = [\"v2\", \\\n",
        "                \"v_parameterization\", \\\n",
        "                \"network_dim\", \\\n",
        "                \"network_alpha\", \\\n",
        "                \"network_module\", \\\n",
        "                \"network_weights\", \\\n",
        "                \"network_train_on\", \\\n",
        "                \"unet_lr\", \\\n",
        "                \"text_encoder_lr\", \\\n",
        "                \"no_metadata\", \\\n",
        "                \"training_comment\", \\\n",
        "                \"lr_scheduler\", \\\n",
        "                \"lr_scheduler_num_cycles\", \\\n",
        "                \"lr_scheduler_power\", \\\n",
        "                \"pretrained_model_name_or_path\", \\\n",
        "                \"vae\", \\\n",
        "                \"caption_extension\", \\\n",
        "                \"train_folder_directory\", \\\n",
        "                \"reg_folder_directory\", \\\n",
        "                \"output_dir\", \\\n",
        "                \"prior_loss_weight\", \\\n",
        "                \"resume_path\", \\\n",
        "                \"project_name\", \\\n",
        "                \"mixed_precision\", \\\n",
        "                \"save_precision\", \\\n",
        "                \"save_n_epochs_type\", \\\n",
        "                \"save_n_epochs_type_value\", \\\n",
        "                \"save_model_as\", \\\n",
        "                \"resolution\", \\\n",
        "                \"train_batch_size\", \\\n",
        "                \"max_token_length\", \\\n",
        "                \"use_8bit_adam\", \\\n",
        "                \"num_epochs\", \\\n",
        "                \"seed\", \\\n",
        "                \"gradient_checkpointing\", \\\n",
        "                \"gradient_accumulation_steps\", \\\n",
        "                \"clip_skip\", \\\n",
        "                \"logging_dir\", \\\n",
        "                \"log_prefix\", \\\n",
        "                \"additional_argument\"]\n",
        "\n",
        "if print_hyperparameter:\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Hyperparameter\", \"Value\"]\n",
        "    for params in debug_params:\n",
        "        if params != \"\":\n",
        "            if globals()[params] == \"\":\n",
        "                value = \"False\"\n",
        "            else:\n",
        "                value = globals()[params]\n",
        "            table.add_row([params, value])\n",
        "    table.align = \"l\"\n",
        "    print(table)\n",
        "\n",
        "    arg_list = train_command.split()\n",
        "    mod_train_command = {'command': arg_list}\n",
        "    \n",
        "    train_folder = os.path.dirname(output_dir)\n",
        "    \n",
        "    # save the YAML string to a file\n",
        "    with open(str(train_folder)+'/dreambooth_lora_cmd.yaml', 'w') as f:\n",
        "        yaml.dump(mod_train_command, f)\n",
        "\n",
        "f = open(\"./train.sh\", \"w\")\n",
        "f.write(train_command)\n",
        "f.close()\n",
        "!chmod +x ./train.sh\n",
        "!./train.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reMcN0bM_o53"
      },
      "source": [
        "# VI. 테스트\n",
        "network_weights: 테스트 사용 Lora(경로 및 이름), 위에서 만든 결과물 지정하게 함<br>\n",
        "network_mul: 테스트 수행시의 Lora 가중치<br>\n",
        "<br>\n",
        "v2: SD v2 모델 여부<br>\n",
        "v_parameterization: 위와 동일<br><br>\n",
        "prompt: 테스트 프롬포트<br>\n",
        "negative: 테스트 네거티브<br>\n",
        "model: 테스트 수행 모델, 학습에 사용한 모델 지정하게 함<br>\n",
        "vae: 테스트 수행 VAE<br>\n",
        "outdir: 출력 경로<br><br>\n",
        "scale: 스케일<br>\n",
        "sampler: 샘플러, \"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\" 중 기입<br>\n",
        "steps: 수행 스텝<br>\n",
        "precision: fp16 쓰자\n",
        "width: 가로 해상도<br>\n",
        "height: 세로 해상도<br>\n",
        "images_per_prompt: 배치 수<br>\n",
        "batch_size: 배치 크기<br>\n",
        "clip_skip: 클립 스킵<br>\n",
        "seed: 시드, -1이면 랜덤<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1Yyq5G0vl_y"
      },
      "outputs": [],
      "source": [
        "network_weights = output_dir +'/'+ project_name + \".\" + save_model_as\n",
        "network_mul = 0.6\n",
        "\n",
        "\n",
        "v2 = False\n",
        "v_parameterization = False\n",
        "\n",
        "prompt = \"masterpiece, best quality, 1girl, blue archive, sunaookami shiroko, halo, animal ears, girl on top, smirk\"\n",
        "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type: \"string\"}\n",
        "model = pretrained_model_name_or_path\n",
        "vae = vae\n",
        "outdir = \"/workspace/repository/tmp\"\n",
        "\n",
        "scale = 12\n",
        "sampler = \"ddim\"\n",
        "steps = 28\n",
        "precision = \"fp16\"\n",
        "width = 512\n",
        "height = 768\n",
        "images_per_prompt = 4\n",
        "batch_size = 4\n",
        "clip_skip = 2\n",
        "seed = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FKBrTDPrcNjP",
        "outputId": "8c74d04d-fcab-4718-baf5-88226d0868ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/repository/kohya-trainer\n",
            "2023-02-03 07:26:42.743161: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-03 07:26:43.229874: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-02-03 07:26:44.962536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-02-03 07:26:44.962777: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-02-03 07:26:44.962815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'logit_scale', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading text encoder: <All keys matched successfully>\n",
            "Replace CrossAttention.forward to use NAI style Hypernetwork and xformers\n",
            "loading tokenizer\n",
            "import network module: networks.lora\n",
            "load network weights from: /workspace/repository/dreambooth/output/BA_shiroko.safetensors\n",
            "metadata for: /workspace/repository/dreambooth/output/BA_shiroko.safetensors: {'ss_batch_size_per_device': '4', 'ss_bucket_info': '{\"buckets\": {\"0\": {\"resolution\": [256, 832], \"count\": 0}, \"1\": {\"resolution\": [256, 896], \"count\": 0}, \"2\": {\"resolution\": [256, 960], \"count\": 0}, \"3\": {\"resolution\": [256, 1024], \"count\": 0}, \"4\": {\"resolution\": [320, 704], \"count\": 70}, \"5\": {\"resolution\": [320, 768], \"count\": 0}, \"6\": {\"resolution\": [384, 640], \"count\": 180}, \"7\": {\"resolution\": [448, 576], \"count\": 130}, \"8\": {\"resolution\": [512, 512], \"count\": 10}, \"9\": {\"resolution\": [576, 448], \"count\": 10}, \"10\": {\"resolution\": [640, 384], \"count\": 0}, \"11\": {\"resolution\": [704, 320], \"count\": 0}, \"12\": {\"resolution\": [768, 320], \"count\": 0}, \"13\": {\"resolution\": [832, 256], \"count\": 0}, \"14\": {\"resolution\": [896, 256], \"count\": 0}, \"15\": {\"resolution\": [960, 256], \"count\": 0}, \"16\": {\"resolution\": [1024, 256], \"count\": 0}}, \"mean_img_ar_error\": 0.04568883450503054}', 'ss_cache_latents': 'True', 'ss_clip_skip': '2', 'ss_color_aug': 'False', 'ss_dataset_dirs': '{\"10_BA_shiroko 1girl\": {\"n_repeats\": 10, \"img_count\": 40}}', 'ss_enable_bucket': 'True', 'ss_epoch': '10', 'ss_flip_aug': 'False', 'ss_full_fp16': 'False', 'ss_gradient_accumulation_steps': '1', 'ss_gradient_checkpointing': 'False', 'ss_keep_tokens': 'None', 'ss_learning_rate': '2e-06', 'ss_lr_scheduler': 'constant', 'ss_lr_warmup_steps': '0', 'ss_max_bucket_reso': '1024', 'ss_max_token_length': '225', 'ss_max_train_steps': '1020', 'ss_min_bucket_reso': '256', 'ss_mixed_precision': 'fp16', 'ss_network_alpha': '128.0', 'ss_network_dim': '128', 'ss_network_module': 'networks.lora', 'ss_new_sd_model_hash': '0873291ac5419eaa7a18726e8841ce0f15f701ace29e0183c47efad2018900a4', 'ss_num_batches_per_epoch': '102', 'ss_num_epochs': '10', 'ss_num_reg_images': '0', 'ss_num_train_images': '400', 'ss_output_name': 'BA_shiroko', 'ss_random_crop': 'False', 'ss_reg_dataset_dirs': '{\"1_1girl\": {\"n_repeats\": 1, \"img_count\": 0}}', 'ss_resolution': '(512, 512)', 'ss_sd_model_hash': 'a87fd7da', 'ss_sd_model_name': 'AbyssOrangeMix2_nsfw.safetensors', 'ss_seed': 'None', 'ss_session_id': '1087783046', 'ss_shuffle_caption': 'True', 'ss_text_encoder_lr': '5e-05', 'ss_total_batch_size': '4', 'ss_training_comment': 'training_comment', 'ss_training_started_at': '1675400011.7560575', 'ss_unet_lr': '0.0001', 'ss_v2': 'False', 'sshs_legacy_hash': '7eefaa92', 'sshs_model_hash': 'b10e353d7373397aa676c21f051e63ac82373a84b58c50748780bc5fd322c32a'}\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "weights are loaded: <All keys matched successfully>\n",
            "gen_img_diffusers.py:448: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
            "  \"_class_name\": \"DDIMScheduler\",\n",
            "  \"_diffusers_version\": \"0.10.2\",\n",
            "  \"beta_end\": 0.012,\n",
            "  \"beta_schedule\": \"scaled_linear\",\n",
            "  \"beta_start\": 0.00085,\n",
            "  \"clip_sample\": true,\n",
            "  \"num_train_timesteps\": 1000,\n",
            "  \"prediction_type\": \"epsilon\",\n",
            "  \"set_alpha_to_one\": true,\n",
            "  \"steps_offset\": 0,\n",
            "  \"trained_betas\": null\n",
            "}\n",
            " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
            "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "gen_img_diffusers.py:461: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
            "  \"_class_name\": \"DDIMScheduler\",\n",
            "  \"_diffusers_version\": \"0.10.2\",\n",
            "  \"beta_end\": 0.012,\n",
            "  \"beta_schedule\": \"scaled_linear\",\n",
            "  \"beta_start\": 0.00085,\n",
            "  \"clip_sample\": true,\n",
            "  \"num_train_timesteps\": 1000,\n",
            "  \"prediction_type\": \"epsilon\",\n",
            "  \"set_alpha_to_one\": true,\n",
            "  \"steps_offset\": 1,\n",
            "  \"trained_betas\": null\n",
            "}\n",
            " has not set the configuration `clip_sample`. `clip_sample` should be set to False in the configuration file. Please make sure to update the config accordingly as not setting `clip_sample` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
            "  deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "pipeline is ready.\n",
            "iteration 1/1\n",
            "prompt 1/1: masterpiece, best quality, 1girl, blue archive, sunaookami shiroko, halo, animal ears, girl on top, smirk\n",
            "negative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n",
            "100%|███████████████████████████████████████████| 28/28 [00:31<00:00,  1.12s/it]\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# 실제 수행\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "\n",
        "final_prompt = f\"{prompt} --n {negative}\"\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "!python gen_img_diffusers.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --network_module={network_module} \\\n",
        "  --network_weight={network_weights} \\\n",
        "  --network_mul={network_mul} \\\n",
        "  --ckpt={model} \\\n",
        "  --outdir={outdir} \\\n",
        "  --xformers \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  --{precision} \\\n",
        "  --W={width} \\\n",
        "  --H={height} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  --scale={scale} \\\n",
        "  --sampler={sampler} \\\n",
        "  --steps={steps} \\\n",
        "  --max_embeddings_multiples=3 \\\n",
        "  --batch_size={batch_size} \\\n",
        "  --images_per_prompt={images_per_prompt} \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --prompt=\"{final_prompt}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF7hKqBpvl_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}